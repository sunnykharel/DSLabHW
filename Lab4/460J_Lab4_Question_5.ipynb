{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "460J Lab4: Question 5",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c0TcgTedHFl"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsLKM0_sDe9C"
      },
      "source": [
        "##Question 5:\n",
        "Assuming previous tutorial has been done. We will start with our base CNN from the \"What is torch.nn\" tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEITjf-cD8dV"
      },
      "source": [
        "### Loading MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5E4scKmD3J4",
        "outputId": "c90e3051-9773-4b18-ae61-6ce1fe9354b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Loading Dataset libraries\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import pickle\n",
        "import gzip\n",
        "# Computional and Graphical libraries\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "import torch\n",
        "# Debugger Library\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "DATA_PATH = Path(\"data\")\n",
        "PATH = DATA_PATH / \"mnist\"\n",
        "\n",
        "PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "URL = \"http://deeplearning.net/data/mnist/\"\n",
        "FILENAME = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (PATH / FILENAME).exists():\n",
        "        content = requests.get(URL + FILENAME).content\n",
        "        (PATH / FILENAME).open(\"wb\").write(content)\n",
        "\n",
        "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
        "  \n",
        "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
        "print(x_train.shape)\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMcTQm2KcsBR"
      },
      "source": [
        "### Is GPU availble?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UzDQRUkdEer",
        "outputId": "d0a90b8e-eaf5-4b5d-be98-7c500e5b84fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "dev = torch.device(\n",
        "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMX9BjCDO-iJ"
      },
      "source": [
        "### Classes and Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZCog-fXHVDG"
      },
      "source": [
        "# Training and Validation Datasets/DataLoaders Libraries\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "# Optim and NN libraries\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_data(train_ds, valid_ds, bs):\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "        DataLoader(valid_ds, batch_size=bs * 2),\n",
        "    )\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)\n",
        "\n",
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        val_acc = sum(accuracy(model(xb), yb) for xb, yb in valid_dl)    #valid_loss / len(valid_dl)\n",
        "\n",
        "        val_acc = val_acc.cpu().detach().numpy() \n",
        "        print(epoch, val_loss, val_acc / len(valid_dl))\n",
        "\n",
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
        "\n",
        "class WrappedDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))\n",
        "\n",
        "# Accuracy check from Validation Test.\n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eSN4xwSlH3"
      },
      "source": [
        "### Initial Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XzHWMwESqht"
      },
      "source": [
        "bs = 64  # batch size\n",
        "lr = 0.1  # learning rate\n",
        "epochs = 2  # how many epochs to train for\n",
        "a = np.zeros((20, 10), dtype=(float,5))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjbN1m4Hes_"
      },
      "source": [
        "### Training and Validation Datasets/DataLoaders \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtFZG5JNPmZ1"
      },
      "source": [
        "train_ds = TensorDataset(x_train, y_train)\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb_1kpsPQVeu"
      },
      "source": [
        "### Model and Optim (Use to do Foward Step)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_2xEVWOPfXY"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "model.to(dev)\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zlVNqLRQzrX"
      },
      "source": [
        "### Training of Model. Outputs Validation Loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90d4ijwtQ7Qu",
        "outputId": "93e221ac-2891-463e-8ccf-ad3ad748ec4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.339774385368824 0.8920094936708861\n",
            "1 0.21009030851125718 0.9386867088607594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VtIxe6hx_q_"
      },
      "source": [
        "### Testing different learning rate and momentum values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q55lEp7YyRmF",
        "outputId": "b8d65788-5e05-495f-a238-3866294d98ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, mat, lr, momentum):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "            )\n",
        "\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        val_acc = sum(accuracy(model(xb), yb) for xb, yb in valid_dl)    #valid_loss / len(valid_dl)\n",
        "        val_acc = val_acc.cpu().detach().numpy() / len(valid_dl)\n",
        "        mat_data = (epoch, lr, momentum, val_loss, val_acc)\n",
        "        mat[int(((lr*20)-2)+epoch)][int(momentum*10)] = mat_data\n",
        "        print(epoch, val_loss, val_acc)\n",
        "    return mat\n",
        "################################################################################\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "model.to(dev)\n",
        "for x in range(10):           # Varying for LR from 0.1 to 1.0  \n",
        "  lr = (x+1)/10\n",
        "  for y in range(10):         # Varying for Momentum 0.0 to 0.9\n",
        "    momentum = y/10\n",
        "    opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    print(lr, momentum)\n",
        "    a = fit(epochs, model, loss_func, opt, train_dl, valid_dl, a, lr, momentum)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1 0.0\n",
            "0 1.4863000289916992 0.45905854430379744\n",
            "1 1.1271489278793334 0.6269778481012658\n",
            "0.1 0.1\n",
            "0 0.6791762075424195 0.7776898734177216\n",
            "1 0.6144604323863984 0.7991495253164557\n",
            "0.1 0.2\n",
            "0 0.3710880497455597 0.8862737341772152\n",
            "1 0.37908401839733125 0.8816257911392406\n",
            "0.1 0.3\n",
            "0 0.2817768128156662 0.9165348101265823\n",
            "1 0.2684560010433197 0.9212816455696202\n",
            "0.1 0.4\n",
            "0 0.25333035026788714 0.9282041139240507\n",
            "1 0.2586450876951218 0.9212816455696202\n",
            "0.1 0.5\n",
            "0 0.4873069658279419 0.8554193037974683\n",
            "1 0.28710642221570015 0.912381329113924\n",
            "0.1 0.6\n",
            "0 0.24276638667583467 0.9288963607594937\n",
            "1 0.18893379352390766 0.9445213607594937\n",
            "0.1 0.7\n",
            "0 0.22176867967247962 0.9373022151898734\n",
            "1 0.19824675492346286 0.9425435126582279\n",
            "0.1 0.8\n",
            "0 0.18635324544012546 0.9465981012658228\n",
            "1 0.1841644081056118 0.9479825949367089\n",
            "0.1 0.9\n",
            "0 0.16314728631675243 0.9553006329113924\n",
            "1 0.18670903607010841 0.9434335443037974\n",
            "0.2 0.0\n",
            "0 0.18890684643387795 0.9449169303797469\n",
            "1 0.13567996456772088 0.9623219936708861\n",
            "0.2 0.1\n",
            "0 0.13560908826291562 0.9614319620253164\n",
            "1 0.13865472483336924 0.9605419303797469\n",
            "0.2 0.2\n",
            "0 0.143240681129694 0.9605419303797469\n",
            "1 0.13303159916996957 0.9636075949367089\n",
            "0.2 0.3\n",
            "0 0.1266035877585411 0.9653876582278481\n",
            "1 0.14329623847603798 0.9594541139240507\n",
            "0.2 0.4\n",
            "0 0.1692038366049528 0.9527294303797469\n",
            "1 0.1319958964318037 0.9638053797468354\n",
            "0.2 0.5\n",
            "0 0.1292878429055214 0.9634098101265823\n",
            "1 0.15155372014343738 0.9547072784810127\n",
            "0.2 0.6\n",
            "0 0.19529641719460486 0.9409612341772152\n",
            "1 0.14397044867277145 0.9579707278481012\n",
            "0.2 0.7\n",
            "0 0.12994211793243884 0.9634098101265823\n",
            "1 0.1205447984278202 0.9657832278481012\n",
            "0.2 0.8\n",
            "0 0.16830104094743728 0.9522349683544303\n",
            "1 0.12831478562355042 0.9618275316455697\n",
            "0.2 0.9\n",
            "0 0.16074707213640213 0.9518393987341772\n",
            "1 0.14952371793985367 0.9547072784810127\n",
            "0.3 0.0\n",
            "0 0.11870482016801834 0.9660799050632911\n",
            "1 0.10813531310111284 0.9692444620253164\n",
            "0.3 0.1\n",
            "0 0.1210621126294136 0.9652887658227848\n",
            "1 0.12322547079324722 0.9623219936708861\n",
            "0.3 0.2\n",
            "0 0.10488862806260586 0.9710245253164557\n",
            "1 0.11765727536678314 0.9677610759493671\n",
            "0.3 0.3\n",
            "0 0.10943166644871236 0.9676621835443038\n",
            "1 0.1112074060536921 0.9690466772151899\n",
            "0.3 0.4\n",
            "0 0.1177787490002811 0.9668710443037974\n",
            "1 0.1046140064574778 0.9691455696202531\n",
            "0.3 0.5\n",
            "0 0.11730511011183262 0.9655854430379747\n",
            "1 0.10850781296938658 0.9671677215189873\n",
            "0.3 0.6\n",
            "0 0.10955738279595971 0.9684533227848101\n",
            "1 0.13792303424477578 0.9615308544303798\n",
            "0.3 0.7\n",
            "0 0.11805085754543543 0.966376582278481\n",
            "1 0.12346452119648457 0.9660799050632911\n",
            "0.3 0.8\n",
            "0 0.12732999440878628 0.964003164556962\n",
            "1 0.1368689041465521 0.960245253164557\n",
            "0.3 0.9\n",
            "0 0.21382727062702178 0.9417523734177216\n",
            "1 0.1464859338864684 0.9604430379746836\n",
            "0.4 0.0\n",
            "0 0.11167256118580698 0.9682555379746836\n",
            "1 0.11187256010994315 0.9684533227848101\n",
            "0.4 0.1\n",
            "0 0.12224916501864791 0.9654865506329114\n",
            "1 0.10780785501897334 0.9686511075949367\n",
            "0.4 0.2\n",
            "0 0.10693383365571499 0.9705300632911392\n",
            "1 0.11585912493914366 0.968057753164557\n",
            "0.4 0.3\n",
            "0 0.11361051338221878 0.9682555379746836\n",
            "1 0.10292249005138875 0.9716178797468354\n",
            "0.4 0.4\n",
            "0 0.10658358543086797 0.9702333860759493\n",
            "1 0.11027148991525174 0.969442246835443\n",
            "0.4 0.5\n",
            "0 0.10987296913899482 0.9679588607594937\n",
            "1 0.10616927255773917 0.9726068037974683\n",
            "0.4 0.6\n",
            "0 0.1370148770544678 0.9642009493670886\n",
            "1 0.11390638188868761 0.9676621835443038\n",
            "0.4 0.7\n",
            "0 0.1279700640693307 0.965684335443038\n",
            "1 0.12727716157063843 0.9648931962025317\n",
            "0.4 0.8\n",
            "0 0.12709024610742928 0.9645965189873418\n",
            "1 0.11362509560994805 0.969442246835443\n",
            "0.4 0.9\n",
            "0 0.1565808003079146 0.9567840189873418\n",
            "1 0.16366909954622388 0.9576740506329114\n",
            "0.5 0.0\n",
            "0 0.11490244829319418 0.9677610759493671\n",
            "1 0.10485177903529257 0.969442246835443\n",
            "0.5 0.1\n",
            "0 0.10865686457138508 0.9700356012658228\n",
            "1 0.10605004655136727 0.9703322784810127\n",
            "0.5 0.2\n",
            "0 0.10234110887050629 0.9707278481012658\n",
            "1 0.11160295746605843 0.9706289556962026\n",
            "0.5 0.3\n",
            "0 0.10941569313211366 0.9709256329113924\n",
            "1 0.10463866094239056 0.9707278481012658\n",
            "0.5 0.4\n",
            "0 0.11531584353912622 0.9695411392405063\n",
            "1 0.10886933372747153 0.9703322784810127\n",
            "0.5 0.5\n",
            "0 0.12324651898182928 0.9678599683544303\n",
            "1 0.11962020082129166 0.9691455696202531\n",
            "0.5 0.6\n",
            "0 0.10589607935778331 0.9705300632911392\n",
            "1 0.11080604134802706 0.9705300632911392\n",
            "0.5 0.7\n",
            "0 0.12489337472445332 0.9679588607594937\n",
            "1 0.12234699442279526 0.9686511075949367\n",
            "0.5 0.8\n",
            "0 0.12919992827028037 0.9651898734177216\n",
            "1 0.15545565596881789 0.9581685126582279\n",
            "0.5 0.9\n",
            "0 0.2169283072590828 0.9428401898734177\n",
            "1 0.20986214278787374 0.9420490506329114\n",
            "0.6 0.0\n",
            "0 0.12065319787710906 0.9662776898734177\n",
            "1 0.11690907094143331 0.9685522151898734\n",
            "0.6 0.1\n",
            "0 0.10886775563322007 0.9693433544303798\n",
            "1 0.1109960504842922 0.9692444620253164\n",
            "0.6 0.2\n",
            "0 0.10812090906258673 0.971123417721519\n",
            "1 0.1299099268297665 0.9644976265822784\n",
            "0.6 0.3\n",
            "0 0.10774974656235427 0.9732990506329114\n",
            "1 0.109175579745695 0.9710245253164557\n",
            "0.6 0.4\n",
            "0 0.12967835938688368 0.9639042721518988\n",
            "1 0.12200700463801623 0.96875\n",
            "0.6 0.5\n",
            "0 0.1196379111431539 0.9684533227848101\n",
            "1 0.12681130185239017 0.9662776898734177\n",
            "0.6 0.6\n",
            "0 0.11307888658381998 0.9712223101265823\n",
            "1 0.11486162941623479 0.9721123417721519\n",
            "0.6 0.7\n",
            "0 0.12463826639540493 0.9684533227848101\n",
            "1 0.1202967264120467 0.9697389240506329\n",
            "0.6 0.8\n",
            "0 0.18207455490157007 0.9553006329113924\n",
            "1 0.15961586581133305 0.9575751582278481\n",
            "0.6 0.9\n",
            "0 0.2331263549953699 0.9370055379746836\n",
            "1 0.3555778139934642 0.9196004746835443\n",
            "0.7 0.0\n",
            "0 0.12838277786765248 0.9630142405063291\n",
            "1 0.11857154126800597 0.9641020569620253\n",
            "0.7 0.1\n",
            "0 0.11722004994563759 0.9665743670886076\n",
            "1 0.11774743082895875 0.9657832278481012\n",
            "0.7 0.2\n",
            "0 0.11728704131096601 0.9676621835443038\n",
            "1 0.11426017383895815 0.9677610759493671\n",
            "0.7 0.3\n",
            "0 0.11367953536324203 0.9683544303797469\n",
            "1 0.11592016365341842 0.968057753164557\n",
            "0.7 0.4\n",
            "0 0.11763628113716841 0.9664754746835443\n",
            "1 0.11609601347595454 0.9667721518987342\n",
            "0.7 0.5\n",
            "0 0.11953599300570786 0.9677610759493671\n",
            "1 0.12701219744682313 0.9634098101265823\n",
            "0.7 0.6\n",
            "0 0.12506643275618554 0.966376582278481\n",
            "1 0.11592082122713328 0.9662776898734177\n",
            "0.7 0.7\n",
            "0 0.15440672205814626 0.9603441455696202\n",
            "1 0.14081323669441045 0.9597507911392406\n",
            "0.7 0.8\n",
            "0 0.13948908088728784 0.9593552215189873\n",
            "1 0.19045573742687702 0.9464992088607594\n",
            "0.7 0.9\n",
            "0 0.2720762509636581 0.9302808544303798\n",
            "1 0.2640857982933521 0.9305775316455697\n",
            "0.8 0.0\n",
            "0 0.14614345505908133 0.9598496835443038\n",
            "1 0.1311442175731063 0.9623219936708861\n",
            "0.8 0.1\n",
            "0 0.13875602120757102 0.9598496835443038\n",
            "1 0.13322172701619567 0.9627175632911392\n",
            "0.8 0.2\n",
            "0 0.12103391640931369 0.9671677215189873\n",
            "1 0.12757812152914702 0.9651898734177216\n",
            "0.8 0.3\n",
            "0 0.13006909120976926 0.9637064873417721\n",
            "1 0.1291356104362756 0.9636075949367089\n",
            "0.8 0.4\n",
            "0 0.12094522495009005 0.9653876582278481\n",
            "1 0.12024650327637791 0.9658821202531646\n",
            "0.8 0.5\n",
            "0 0.12572852872870863 0.9648931962025317\n",
            "1 0.11854034664519131 0.9665743670886076\n",
            "0.8 0.6\n",
            "0 0.12452222608029842 0.9661787974683544\n",
            "1 0.12961134017035364 0.9658821202531646\n",
            "0.8 0.7\n",
            "0 0.12943860225658863 0.9641020569620253\n",
            "1 0.1360952475104481 0.9606408227848101\n",
            "0.8 0.8\n",
            "0 0.15003418617844583 0.9608386075949367\n",
            "1 0.1521322960022837 0.9595530063291139\n",
            "0.8 0.9\n",
            "0 0.3283165837407112 0.9094145569620253\n",
            "1 0.3572272372722626 0.893690664556962\n",
            "0.9 0.0\n",
            "0 0.23423945236206054 0.9349287974683544\n",
            "1 0.21415408178567885 0.9350276898734177\n",
            "0.9 0.1\n",
            "0 0.1971885336071253 0.9427412974683544\n",
            "1 0.2110540614426136 0.9399723101265823\n",
            "0.9 0.2\n",
            "0 0.19980920732021332 0.9411590189873418\n",
            "1 0.20893248408436776 0.9401700949367089\n",
            "0.9 0.3\n",
            "0 0.19360853391885757 0.9433346518987342\n",
            "1 0.17934903336763383 0.9485759493670886\n",
            "0.9 0.4\n",
            "0 0.17535237710177898 0.9510482594936709\n",
            "1 0.262751783297956 0.9265229430379747\n",
            "0.9 0.5\n",
            "0 0.18892815390229226 0.9431368670886076\n",
            "1 0.1643377272516489 0.9517405063291139\n",
            "0.9 0.6\n",
            "0 0.1596179372623563 0.9550039556962026\n",
            "1 0.15983628551363946 0.955498417721519\n",
            "0.9 0.7\n",
            "0 0.17997167784571647 0.948378164556962\n",
            "1 0.16808373177945612 0.9519382911392406\n",
            "0.9 0.8\n",
            "0 0.18387740416526793 0.9471914556962026\n",
            "1 0.198883735665679 0.9449169303797469\n",
            "0.9 0.9\n",
            "0 0.4305316368103027 0.8915150316455697\n",
            "1 0.513553396654129 0.8509691455696202\n",
            "1.0 0.0\n",
            "0 0.20188690141141416 0.9430379746835443\n",
            "1 0.18997681085467338 0.9467958860759493\n",
            "1.0 0.1\n",
            "0 0.18076044195890426 0.947685917721519\n",
            "1 0.17901626200675963 0.9490704113924051\n",
            "1.0 0.2\n",
            "0 0.2164471524477005 0.9428401898734177\n",
            "1 0.17398800148963928 0.9493670886075949\n",
            "1.0 0.3\n",
            "0 0.17637476187944412 0.9482792721518988\n",
            "1 0.17718745999336244 0.9487737341772152\n",
            "1.0 0.4\n",
            "0 0.1699304197192192 0.9525316455696202\n",
            "1 0.16306727154254913 0.9521360759493671\n",
            "1.0 0.5\n",
            "0 0.1781700668990612 0.9470925632911392\n",
            "1 0.17355914738178252 0.9506526898734177\n",
            "1.0 0.6\n",
            "0 0.17900192398428916 0.9473892405063291\n",
            "1 0.1801849832892418 0.9448180379746836\n",
            "1.0 0.7\n",
            "0 0.1967512058854103 0.9419501582278481\n",
            "1 0.25477542139291764 0.9253362341772152\n",
            "1.0 0.8\n",
            "0 0.22830823081731796 0.9333465189873418\n",
            "1 0.2421811589717865 0.9311708860759493\n",
            "1.0 0.9\n",
            "0 3.5250786003112795 0.6431962025316456\n",
            "1 0.7367884540557861 0.7876780063291139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rtgLEiyDVYv"
      },
      "source": [
        "### Graphical Illustration of Accuracy for Varying Values of Learning Rate and Momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9MnQ3BL9kAu"
      },
      "source": [
        "def Largest_Moment(mat, index):\n",
        "  best_Momentum_index = 0;\n",
        "  for x in range(10):\n",
        "    if(mat[index][x][4] > mat[index][best_Momentum_index][4]):\n",
        "      best_Momentum_index = x\n",
        "  return best_Momentum_index\n",
        "\n",
        "mat_lr = np.zeros(20)\n",
        "mat_moment = np.zeros(20)\n",
        "mat_acc = np.zeros(20)\n",
        "\n",
        "for i in range(20):\n",
        "  mat_lr[i] = a[i][0][1]\n",
        "  # Momentum index with greatest Accuracy of a given LR.\n",
        "  large = Largest_Moment(a, i)\n",
        "  mat_moment[i] = a[i][large][2]\n",
        "  mat_acc[i] = a[i][large][4]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k6Tg4YjMgNw"
      },
      "source": [
        "The arrays of LR, Momentum, and Accuracy should be counted in groups of 2. First Value is epoch 1, second value is epoch 2. Then LR/Momentum will increment. For example, below we see that the 8th value in accuracy array is the largest. This corresponds to a learning rate of 0.5 and momentum 0.4 and epoch 1. Loss of valiation sets are also shown as 0.9500714. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGDJF_IWMBWk",
        "outputId": "a097e4d3-fea0-44c9-abe3-651949d66e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(mat_acc)\n",
        "print(mat_moment[10])\n",
        "print(a[10][3])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.95530063 0.94798259 0.96538766 0.96578323 0.97102453 0.96924446\n",
            " 0.97053006 0.9726068  0.97092563 0.97072785 0.97329905 0.97211234\n",
            " 0.96835443 0.96805775 0.96716772 0.96657437 0.95500396 0.95549842\n",
            " 0.95253165 0.95213608]\n",
            "0.3\n",
            "[0.         0.6        0.3        0.10774975 0.97329905]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYtsstdMJF1u",
        "outputId": "d462f312-884f-4d7e-deca-dd8f4314ebe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "source": [
        "fig, ax = pyplot.subplots()\n",
        "ax.plot(mat_lr, mat_moment, 'ro')\n",
        "ax.axis([0, 1, 0, 1])\n",
        "ax.set(xlabel='Learning Rate', ylabel='Momentum Value with Largest Accuracy')\n",
        "ax.grid()\n",
        "pyplot.show()\n",
        "\n",
        "fig, ax = pyplot.subplots()\n",
        "ax.plot(mat_lr, mat_acc, 'ro')\n",
        "ax.axis([0, 1, 0, 1])\n",
        "ax.set(xlabel='Learning Rate', ylabel='Accuracy')\n",
        "ax.grid()\n",
        "pyplot.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6-MGTB7Jr58"
      },
      "source": [
        "From the plot above. We got our best accuracy, 0.97329905 , with a learning rate of 0.6 and a momentum of 0.3. In the graphs above, every LR has two dots since there is 2 epochs."
      ]
    }
  ]
}